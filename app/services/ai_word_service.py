import asyncio
import logging
import time
import json
from collections import defaultdict
from typing import List, Dict, Any

from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import PydanticOutputParser
from sqlmodel import Session, select
from app.models import WordEntry, JapaneseWordMetadata, WordLearningContent, SubtitleSentence
from app.schemas import AIWordEnhanceResponseV2

load_dotenv()

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

PROMPT_V2 = """
ë„ˆëŠ” ì¼ë³¸ì–´ êµìœ¡ ì „ë¬¸ê°€ì´ì ì°½ì˜ì ì¸ ì‘ê°€ì•¼. ì œê³µëœ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•™ìŠµ ì½˜í…ì¸ ë¥¼ ë§Œë“¤ì–´ì¤˜.

[ìˆ˜í–‰ ê³¼ì œ]
1. contextsë¥¼ ì°¸ì¡°í•˜ì—¬ ê° ë‹¨ì–´ì˜ ì •í™•í•œ ì˜ë¯¸ì™€ ì½ê¸°ë¥¼ íŒŒì•…í•´.
2. ê° word_groupsì— ëŒ€í•´, í•´ë‹¹ ê·¸ë£¹ì˜ ë‹¨ì–´ 3ê°œë¥¼ ëª¨ë‘ í¬í•¨í•œ 'ìƒˆë¡œìš´' ì˜ˆë¬¸ì„ í•˜ë‚˜ì”© ì°½ì¡°í•´.

[ì ˆëŒ€ ê·œì¹™]
- ì œê³µëœ contextsì˜ ë¬¸ì¥ì„ ê·¸ëŒ€ë¡œ ë¦¬í„´í•˜ê±°ë‚˜ ë³µì‚¬í•˜ì§€ ë§ˆë¼. (ì €ì‘ê¶Œ ì¤€ìˆ˜)
- ë°˜ë“œì‹œ ìë§‰ ìƒí™©ê³¼ ë‹¤ë¥¸ ê³ ìœ í•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ì„ ë§Œë“¤ì–´ë¼.
- ëª¨ë“  ì½ê¸°ëŠ” íˆë¼ê°€ë‚˜ë¡œë§Œ ì‘ì„±í•´ë¼.

{format_instructions}

[ì…ë ¥ ë°ì´í„°]
{payload}
"""


class AIWordService:
    def __init__(self):
        self.llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
        self.parser = PydanticOutputParser(pydantic_object=AIWordEnhanceResponseV2)
        self.prompt = ChatPromptTemplate.from_template(PROMPT_V2)
        self.semaphore = asyncio.Semaphore(3)  # ë™ì‹œ ì‹¤í–‰ ì œí•œ

    def _chunk_list(self, lst, n):
        for i in range(0, len(lst), n):
            yield lst[i:i + n]

    def _prepare_payload(self, results: List[tuple]) -> Dict[str, Any]:
        """
        DB ì¡°íšŒ ê²°ê³¼(WordEntry, SubtitleSentence)ë¥¼ AI ì „ë‹¬ìš© êµ¬ì¡°ë¡œ ë§¤í•‘
        ë¬¸ì¥ ê°„ ì¸í„°ë¦¬ë¹™ ì „ëµìœ¼ë¡œ ë‹¨ì–´ ê·¸ë£¹í™”
        """
        # 1. ë¬¸ì¥ ì¤‘ë³µ ì œê±° ë° ë‹¨ì–´ ë¶„ë¥˜ (Sentence-based Grouping)
        # ìœ ë‹ˆí¬í•œ ë¬¸ì¥(Context) ì¶”ì¶œ ë° ì§§ì€ ID ë¶€ì—¬
        unique_contexts = {}  # {db_sentence_id: {"id": simple_id, "text": text}}
        sentence_buckets = defaultdict(list)  # {sentence_id: [word_info, ...]}
        context_counter = 1

        # 2. ë‹¨ì–´ë³„ë¡œ ì»¨í…ìŠ¤íŠ¸ ì°¸ì¡° ì •ë³´ ì •ë¦¬
        for word, sentence in results:
            if sentence.id not in unique_contexts:
                unique_contexts[sentence.id] = {
                    "id": context_counter,
                    "text": sentence.sentence_text
                }
                context_counter += 1

            # ë¬¸ì¥ IDë³„ë¡œ ë‹¨ì–´ë¥¼ ë‹´ì•„ë‘¡ë‹ˆë‹¤ (ë°”êµ¬ë‹ˆ ì±„ìš°ê¸°)
            sentence_buckets[sentence.id].append({
                "base_form": word.base_form,
                "context_id": unique_contexts[sentence.id]["id"]
            })

        # 2. ì¸í„°ë¦¬ë¹™(Interleaving) ì „ëµ: ê° ë°”êµ¬ë‹ˆì—ì„œ í•˜ë‚˜ì”© ê³¨ê³ ë£¨ ë½‘ê¸°
        interleaved_words = []
        buckets = list(sentence_buckets.values())

        # ëª¨ë“  ë°”êµ¬ë‹ˆê°€ ë¹Œ ë•Œê¹Œì§€ ëŒì•„ê°€ë©° í•˜ë‚˜ì”© ì¶”ì¶œ
        max_len = max(len(b) for b in buckets)
        for i in range(max_len):
            for bucket in buckets:
                if i < len(bucket):
                    interleaved_words.append(bucket[i])

        # 3. ì„ì¸ ë¦¬ìŠ¤íŠ¸ë¥¼ 3ê°œì”© ì†Œê·¸ë£¹í™”
        group_size = 3
        word_groups = []
        for i in range(0, len(interleaved_words), group_size):
            group_id = (i // group_size) + 1
            word_groups.append({
                "group_id": group_id,
                "words": interleaved_words[i: i + group_size]
            })

        return {
            "contexts": list(unique_contexts.values()),
            "word_groups": word_groups
        }

    async def _process_batch(self, session: Session, chunk: List[tuple], batch_idx: int, total_batches: int):
        """ê°œë³„ ë°°ì¹˜ë¥¼ ì²˜ë¦¬í•˜ê³  ì§„í–‰ í˜„í™©ì„ ë¡œê¹…í•¨"""
        async with self.semaphore:
            batch_start_time = time.time()
            logger.info(f"ğŸš€ [Batch {batch_idx}/{total_batches}] v0.0.2 ì²˜ë¦¬ ì‹œì‘...")

            # 1. ë°ì´í„° ë§¤í•‘ (ì¸í„°ë¦¬ë¹™)
            payload = self._create_interleaved_payload(chunk)

            # 2. AI í˜¸ì¶œ
            input_data = self.prompt.format_messages(
                payload=json.dumps(payload, ensure_ascii=False),
                format_instructions=self.parser.get_format_instructions()
            )

            try:
                response = await self.llm.ainvoke(input_data)
                ai_data = self.parser.parse(response.content)

                # 3. DB ì €ì¥
                success_count = self._save_results(session, chunk, ai_data)

                duration = time.time() - batch_start_time
                logger.info(f"âœ… [Batch {batch_idx}/{total_batches}] ì™„ë£Œ ({success_count}ê°œ) - {duration:.2f}s")
                return success_count
            except Exception as e:
                logger.error(f"âŒ [Batch {batch_idx}/{total_batches}] ì—ëŸ¬: {str(e)}")
                return 0

    def _create_interleaved_payload(self, chunk: List[tuple]) -> Dict[str, Any]:
        """chunk ë‚´ë¶€ì˜ ë‹¨ì–´ë“¤ì„ ë¬¸ì¥ë³„ë¡œ ì„ì–´ ê·¸ë£¹í™”"""
        unique_contexts = {}
        sentence_buckets = defaultdict(list)
        context_counter = 1

        for word, sentence_id, sentence_text in chunk:
            if sentence_id not in unique_contexts:
                unique_contexts[sentence_id] = {"id": context_counter, "text": sentence_text}
                context_counter += 1

            sentence_buckets[sentence_id].append({
                "word_id": word.id,
                "base_form": word.base_form,
                "context_id": unique_contexts[sentence_id]["id"]
            })

        interleaved = []
        buckets = list(sentence_buckets.values())
        max_len = max(len(b) for b in buckets)
        for i in range(max_len):
            for bucket in buckets:
                if i < len(bucket): interleaved.append(bucket[i])

        word_groups = []
        for i in range(0, len(interleaved), 3):
            word_groups.append({
                "group_id": (i // 3) + 1,
                "words": interleaved[i: i + 3]
            })

        return {"contexts": list(unique_contexts.values()), "word_groups": word_groups}


    async def enhance_words_v2(self, session: Session, results: List[tuple]):
        # 1. ë°ì´í„° ë§¤í•‘ (ì•„ê¹Œ ë§Œë“  ë¡œì§)
        payload = self._prepare_payload(results)

        # 2. í”„ë¡¬í”„íŠ¸ ì£¼ì…
        input_data = self.prompt_template.format_messages(
            payload=json.dumps(payload, ensure_ascii=False),
            format_instructions=self.parser.get_format_instructions()
        )

        # 3. AI í˜¸ì¶œ
        try:
            response = await self.llm.ainvoke(input_data)
            ai_data = self.parser.parse(response.content)

            # 4. DB ì €ì¥ (ì´ ë¶€ë¶„ì´ ë‹¤ìŒ ê³ ë¹„!)
            # ai_data.word_definitions -> JapaneseWordMetadata ì—…ë°ì´íŠ¸
            # ai_data.examples -> WordLearningContent ìƒì„± ë° ì €ì¥
            return ai_data
        except Exception as e:
            logger.error(f"AI ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return None

    def _save_results(self, session: Session, chunk: List[tuple], ai_data: AIWordEnhanceResponseV2):
        word_entry_map = {word.id: word for word, _, _ in chunk}

        # 1. í˜„ì¬ ë°°ì¹˜ì— í¬í•¨ëœ WordEntry ê°ì²´ë“¤ì„ ID ê¸°ë°˜ Mapìœ¼ë¡œ ë³€í™˜
        word_entry_map = {word.id: word for word, _, _ in chunk}

        # 2. AIê°€ ì‘ë‹µí•œ ì˜ˆë¬¸ë“¤ì„ word_id ê¸°ë°˜ìœ¼ë¡œ ì—­ë§¤í•‘ (ì¤‘ìš”!)
        # ì–´ë–¤ word_idê°€ ì–´ë–¤ ì˜ˆë¬¸ì„ ê°€ì ¸ì•¼ í•˜ëŠ”ì§€ Map ìƒì„±
        # {word_id: GroupedExample}
        word_to_example_map = {}
        for ex in ai_data.examples:
            for w_id in ex.word_ids:
                word_to_example_map[w_id] = ex

        success_count = 0

        # 3. AI ì‘ë‹µ ë°ì´í„° ìˆœíšŒ ë° DB ë°˜ì˜
        for def_res in ai_data.word_definitions:
            word_id = def_res.word_id
            word_entry = word_entry_map.get(word_id)

            if not word_entry:
                logger.warning(f"âš ï¸ AIê°€ ë³´ë‚¸ word_id {word_id}ë¥¼ í˜„ì¬ ë°°ì¹˜ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                continue

            # [A] ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ (JapaneseWordMetadata)
            word_entry.japanese_metadata.reading = def_res.reading
            word_entry.japanese_metadata.jlpt_level = def_res.jlpt_level

            # [B] í•™ìŠµ ì½˜í…ì¸  ì €ì¥ (WordLearningContent)
            # ì—­ë§¤í•‘ëœ ë§µì—ì„œ í•´ë‹¹ ë‹¨ì–´ì˜ ì˜ˆë¬¸ì„ ì°¾ì•„ì˜´
            matching_ex = word_to_example_map.get(word_id)

            if matching_ex:
                content = WordLearningContent(
                    word_entry_id=word_entry.id,
                    meaning=def_res.meaning,
                    usage_tip=f"JLPT {def_res.jlpt_level} ìˆ˜ì¤€",
                    generated_example={
                        "ja": matching_ex.new_sentence_ja,
                        "ko": matching_ex.new_sentence_ko
                    }
                )
                session.add(content)
                success_count += 1

        return success_count

    async def enhance_words_hybrid(self, session: Session, subtitle_id: int, batch_size: int = 15):
        start_time = time.time()
        # ì¿¼ë¦¬ ìˆ˜ì •: sentence_idë¥¼ í•¨ê»˜ ê°€ì ¸ì™€ì„œ ì¸í„°ë¦¬ë¹™ì— í™œìš©
        statement = (
            select(WordEntry, SubtitleSentence.id, SubtitleSentence.sentence_text)
            .join(JapaneseWordMetadata)
            .join(SubtitleSentence, WordEntry.first_occurrence_id == SubtitleSentence.id)
            .where(WordEntry.subtitle_id == subtitle_id, WordEntry.is_valid == True,
                   JapaneseWordMetadata.jlpt_level == "WAIT")
        )
        results = session.exec(statement).all()
        if not results: return 0

        chunks = list(self._chunk_list(results, batch_size))
        tasks = [self._process_batch(session, chunk, i + 1, len(chunks)) for i, chunk in enumerate(chunks)]

        success_counts = await asyncio.gather(*tasks)
        session.commit()

        logger.info(f"ğŸ‰ ì‘ì—… ì™„ë£Œ: {sum(success_counts)}ê°œ ë‹¨ì–´ ì„±ê³µ ({time.time() - start_time:.2f}s)")
        return sum(success_counts)