import asyncio
import logging
import time
import json
from collections import defaultdict
from typing import List, Dict, Any

from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import PydanticOutputParser
from sqlmodel import Session, select
from app.models import WordEntry, JapaneseWordMetadata, WordLearningContent, SubtitleSentence, ExampleSentence, \
    ExampleTranslation, SubtitleTranslation
from app.schemas import AIWordEnhanceResponseV2, AIWordEnhanceResponseV4

load_dotenv()

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

PROMPT_V2 = """
ë„ˆëŠ” ì¼ë³¸ì–´ êµìœ¡ ì „ë¬¸ê°€ì´ì ì°½ì˜ì ì¸ ì‘ê°€ì•¼. ì œê³µëœ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•™ìŠµ ì½˜í…ì¸ ë¥¼ ë§Œë“¤ì–´ì¤˜.

[ìˆ˜í–‰ ê³¼ì œ]
1. contextsë¥¼ ì°¸ì¡°í•˜ì—¬ ê° ë‹¨ì–´ì˜ ì •í™•í•œ ì˜ë¯¸ì™€ ì½ê¸°ë¥¼ íŒŒì•…í•´.
2. ê° word_groupsì— ëŒ€í•´, í•´ë‹¹ ê·¸ë£¹ì˜ ë‹¨ì–´ 3ê°œë¥¼ ëª¨ë‘ í¬í•¨í•œ 'ìƒˆë¡œìš´' ì˜ˆë¬¸ì„ í•˜ë‚˜ì”© ì°½ì¡°í•´.

[ì ˆëŒ€ ê·œì¹™]
- ì œê³µëœ contextsì˜ ë¬¸ì¥ì„ ê·¸ëŒ€ë¡œ ë¦¬í„´í•˜ê±°ë‚˜ ë³µì‚¬í•˜ì§€ ë§ˆë¼. (ì €ì‘ê¶Œ ì¤€ìˆ˜)
- ë°˜ë“œì‹œ ìë§‰ ìƒí™©ê³¼ ë‹¤ë¥¸ ê³ ìœ í•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ì„ ë§Œë“¤ì–´ë¼.
- ëª¨ë“  ì½ê¸°ëŠ” íˆë¼ê°€ë‚˜ë¡œë§Œ ì‘ì„±í•´ë¼.

{format_instructions}

[ì…ë ¥ ë°ì´í„°]
{payload}
"""

PROMPT_V3 = """
ë„ˆëŠ” ê¸€ë¡œë²Œ ì¼ë³¸ì–´ êµìœ¡ ì „ë¬¸ê°€ì´ì ì „ë¬¸ ë²ˆì—­ê°€ì•¼. 
ì œê³µëœ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤êµ­ì–´ í•™ìŠµ ì½˜í…ì¸ ë¥¼ ìƒì„±í•´ì¤˜.

[ìˆ˜í–‰ ê³¼ì œ]
1. Context Translation: ì œê³µëœ `contexts` ë¦¬ìŠ¤íŠ¸ì˜ ëª¨ë“  ë¬¸ì¥ì„ í•œêµ­ì–´ë¡œ ì •í™•í•˜ê²Œ ë²ˆì—­í•´.
2. Word Definition: ê° ë‹¨ì–´ì˜ ì˜ë¯¸ë¥¼ contexts ë¬¸ë§¥ì— ë§ê²Œ í’€ì´í•´.
3. Creative Example: `word_groups`ì˜ ë‹¨ì–´ë“¤ì„ í™œìš©í•´ ì €ì‘ê¶Œ ì—†ëŠ” ê³ ìœ í•œ ìƒˆ ë¬¸ì¥ì„ ì°½ì‘í•˜ê³  í•´ì„ì„ ë‹¬ì•„ì¤˜.

[ì ˆëŒ€ ê·œì¹™]
- ì›ë³¸ ìë§‰ì„ ê·¸ëŒ€ë¡œ ì˜ˆë¬¸ìœ¼ë¡œ ì“°ì§€ ë§ˆë¼.
- ëª¨ë“  `word_id`ì™€ `context_id`ë¥¼ ì •í™•íˆ ë§¤ì¹­í•˜ì—¬ ë¦¬í„´í•´ë¼.

{format_instructions}

[ì…ë ¥ ë°ì´í„°]
{payload}
"""


class AIWordService:
    def __init__(self):
        self.llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
        self.parser = PydanticOutputParser(pydantic_object=AIWordEnhanceResponseV4)
        self.prompt = ChatPromptTemplate.from_template(PROMPT_V3)
        self.semaphore = asyncio.Semaphore(3)  # ë™ì‹œ ì‹¤í–‰ ì œí•œ

    def _chunk_list(self, lst, n):
        for i in range(0, len(lst), n):
            yield lst[i:i + n]

    def _prepare_payload(self, results: List[tuple]) -> Dict[str, Any]:
        """
        DB ì¡°íšŒ ê²°ê³¼(WordEntry, SubtitleSentence)ë¥¼ AI ì „ë‹¬ìš© êµ¬ì¡°ë¡œ ë§¤í•‘
        ë¬¸ì¥ ê°„ ì¸í„°ë¦¬ë¹™ ì „ëµìœ¼ë¡œ ë‹¨ì–´ ê·¸ë£¹í™”
        """
        # 1. ë¬¸ì¥ ì¤‘ë³µ ì œê±° ë° ë‹¨ì–´ ë¶„ë¥˜ (Sentence-based Grouping)
        # ìœ ë‹ˆí¬í•œ ë¬¸ì¥(Context) ì¶”ì¶œ ë° ì§§ì€ ID ë¶€ì—¬
        unique_contexts = {}  # {db_sentence_id: {"id": simple_id, "text": text}}
        sentence_buckets = defaultdict(list)  # {sentence_id: [word_info, ...]}
        context_counter = 1

        # 2. ë‹¨ì–´ë³„ë¡œ ì»¨í…ìŠ¤íŠ¸ ì°¸ì¡° ì •ë³´ ì •ë¦¬
        for word, sentence in results:
            if sentence.id not in unique_contexts:
                unique_contexts[sentence.id] = {
                    "id": context_counter,
                    "text": sentence.sentence_text
                }
                context_counter += 1

            # ë¬¸ì¥ IDë³„ë¡œ ë‹¨ì–´ë¥¼ ë‹´ì•„ë‘¡ë‹ˆë‹¤ (ë°”êµ¬ë‹ˆ ì±„ìš°ê¸°)
            sentence_buckets[sentence.id].append({
                "base_form": word.base_form,
                "context_id": unique_contexts[sentence.id]["id"]
            })

        # 2. ì¸í„°ë¦¬ë¹™(Interleaving) ì „ëµ: ê° ë°”êµ¬ë‹ˆì—ì„œ í•˜ë‚˜ì”© ê³¨ê³ ë£¨ ë½‘ê¸°
        interleaved_words = []
        buckets = list(sentence_buckets.values())

        # ëª¨ë“  ë°”êµ¬ë‹ˆê°€ ë¹Œ ë•Œê¹Œì§€ ëŒì•„ê°€ë©° í•˜ë‚˜ì”© ì¶”ì¶œ
        max_len = max(len(b) for b in buckets)
        for i in range(max_len):
            for bucket in buckets:
                if i < len(bucket):
                    interleaved_words.append(bucket[i])

        # 3. ì„ì¸ ë¦¬ìŠ¤íŠ¸ë¥¼ 3ê°œì”© ì†Œê·¸ë£¹í™”
        group_size = 3
        word_groups = []
        for i in range(0, len(interleaved_words), group_size):
            group_id = (i // group_size) + 1
            word_groups.append({
                "group_id": group_id,
                "words": interleaved_words[i: i + group_size]
            })

        return {
            "contexts": list(unique_contexts.values()),
            "word_groups": word_groups
        }

    async def _process_batch(self, session: Session, chunk: List[tuple], batch_idx: int, total_batches: int):
        """ê°œë³„ ë°°ì¹˜ë¥¼ ì²˜ë¦¬í•˜ê³  ì§„í–‰ í˜„í™©ì„ ë¡œê¹…í•¨"""
        async with self.semaphore:
            batch_start_time = time.time()
            logger.info(f"ğŸš€ [Batch {batch_idx}/{total_batches}] v0.0.2 ì²˜ë¦¬ ì‹œì‘...")

            # 1. ë°ì´í„° ë§¤í•‘ (ì¸í„°ë¦¬ë¹™)
            payload = self._create_interleaved_payload(chunk)

            # 2. AI í˜¸ì¶œ
            input_data = self.prompt.format_messages(
                payload=json.dumps(payload, ensure_ascii=False),
                format_instructions=self.parser.get_format_instructions()
            )

            try:
                response = await self.llm.ainvoke(input_data)
                ai_data = self.parser.parse(response.content)

                # 3. DB ì €ì¥
                success_count = self._save_results(session, chunk, ai_data)

                duration = time.time() - batch_start_time
                logger.info(f"âœ… [Batch {batch_idx}/{total_batches}] ì™„ë£Œ ({success_count}ê°œ) - {duration:.2f}s")
                return success_count
            except Exception as e:
                logger.error(f"âŒ [Batch {batch_idx}/{total_batches}] ì—ëŸ¬: {str(e)}")
                return 0

    def _create_interleaved_payload(self, chunk: List[tuple]) -> Dict[str, Any]:
        """chunk ë‚´ë¶€ì˜ ë‹¨ì–´ë“¤ì„ ë¬¸ì¥ë³„ë¡œ ì„ì–´ ê·¸ë£¹í™”"""
        unique_contexts = {}
        sentence_buckets = defaultdict(list)
        # {simple_id: db_id} ì—­ë§¤í•‘ìš© ë”•ì…”ë„ˆë¦¬
        id_mapping = {}
        context_counter = 1

        for word, sentence_id, sentence_text in chunk:
            if sentence_id not in unique_contexts:
                unique_contexts[sentence_id] = {"id": context_counter, "text": sentence_text}
                # ì—­ë§¤í•‘ ì •ë³´ ì €ì¥
                id_mapping[context_counter] = sentence_id
                context_counter += 1

            sentence_buckets[sentence_id].append({
                "word_id": word.id,
                "base_form": word.base_form,
                "context_id": unique_contexts[sentence_id]["id"]
            })

        interleaved = []
        buckets = list(sentence_buckets.values())
        max_len = max(len(b) for b in buckets)
        for i in range(max_len):
            for bucket in buckets:
                if i < len(bucket): interleaved.append(bucket[i])

        word_groups = []
        for i in range(0, len(interleaved), 3):
            word_groups.append({
                "group_id": (i // 3) + 1,
                "words": interleaved[i: i + 3]
            })

        return {
            "payload": {"contexts": list(unique_contexts.values()), "word_groups": word_groups},
            "context_mapping": id_mapping  # ì´ ì •ë³´ê°€ ì €ì¥ ì‹œ í•„ìš”í•¨
        }


    async def enhance_words_v2(self, session: Session, results: List[tuple]):
        # 1. ë°ì´í„° ë§¤í•‘ (ì•„ê¹Œ ë§Œë“  ë¡œì§)
        payload = self._prepare_payload(results)

        # 2. í”„ë¡¬í”„íŠ¸ ì£¼ì…
        input_data = self.prompt_template.format_messages(
            payload=json.dumps(payload, ensure_ascii=False),
            format_instructions=self.parser.get_format_instructions()
        )

        # 3. AI í˜¸ì¶œ
        try:
            response = await self.llm.ainvoke(input_data)
            ai_data = self.parser.parse(response.content)

            # 4. DB ì €ì¥ (ì´ ë¶€ë¶„ì´ ë‹¤ìŒ ê³ ë¹„!)
            # ai_data.word_definitions -> JapaneseWordMetadata ì—…ë°ì´íŠ¸
            # ai_data.examples -> WordLearningContent ìƒì„± ë° ì €ì¥
            return ai_data
        except Exception as e:
            logger.error(f"AI ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return None

    def _save_results(self, session: Session, chunk: List[tuple], ai_data: Any, context_mapping: Dict[int, int]):
        word_entry_map = {word.id: word for word, _, _ in chunk}
        success_count = 0

        # 1. ì›ë³¸ ìë§‰ ë²ˆì—­ ì €ì¥ (SubtitleTranslation)
        for trans in ai_data.subtitle_translations:
            db_sentence_id = context_mapping.get(trans.context_id)
            if db_sentence_id:
                new_sub_trans = SubtitleTranslation(
                    subtitle_sentence_id=db_sentence_id,
                    language_code="ko",
                    translated_text=trans.translation_ko
                )
                session.add(new_sub_trans)

        # 2. ì˜ˆë¬¸ ë°ì´í„° ì²˜ë¦¬ (ExampleSentence & Translation)
        created_example_ids = {}
        for ex in ai_data.examples:
            new_example = ExampleSentence(group_id=ex.group_id, sentence_text=ex.new_sentence_ja)
            session.add(new_example)
            session.flush()  # ID í™•ë³´

            created_example_ids[ex.group_id] = new_example.id
            session.add(ExampleTranslation(
                example_id=new_example.id,
                language_code="ko",
                translated_text=ex.new_sentence_ko
            ))

        # 3. ë‹¨ì–´ë³„ ë©”íƒ€ë°ì´í„° ë° í•™ìŠµ ì½˜í…ì¸  ë§¤í•‘
        for def_res in ai_data.word_definitions:
            word_entry = word_entry_map.get(def_res.word_id)
            if not word_entry: continue

            word_entry.japanese_metadata.reading = def_res.reading
            word_entry.japanese_metadata.jlpt_level = def_res.jlpt_level

            target_group_id = next((ex.group_id for ex in ai_data.examples if def_res.word_id in ex.word_ids), None)
            if target_group_id in created_example_ids:
                session.add(WordLearningContent(
                    word_entry_id=word_entry.id,
                    example_id=created_example_ids[target_group_id],
                    meaning=def_res.meaning,
                    usage_tip=f"JLPT {def_res.jlpt_level} ìˆ˜ì¤€",
                    language_code="ko"
                ))
                success_count += 1
        return success_count

    async def enhance_words_hybrid(self, session: Session, subtitle_id: int, batch_size: int = 15):
        start_time = time.time()
        # ì¿¼ë¦¬ ìˆ˜ì •: sentence_idë¥¼ í•¨ê»˜ ê°€ì ¸ì™€ì„œ ì¸í„°ë¦¬ë¹™ì— í™œìš©
        statement = (
            select(WordEntry, SubtitleSentence.id, SubtitleSentence.sentence_text)
            .join(JapaneseWordMetadata)
            .join(SubtitleSentence, WordEntry.first_occurrence_id == SubtitleSentence.id)
            .where(WordEntry.subtitle_id == subtitle_id, WordEntry.is_valid == True,
                   JapaneseWordMetadata.jlpt_level == "WAIT")
        )
        results = session.exec(statement).all()
        if not results: return 0

        chunks = list(self._chunk_list(results, batch_size))
        tasks = [self._process_batch(session, chunk, i + 1, len(chunks)) for i, chunk in enumerate(chunks)]

        success_counts = await asyncio.gather(*tasks)
        session.commit()

        logger.info(f"ğŸ‰ ì‘ì—… ì™„ë£Œ: {sum(success_counts)}ê°œ ë‹¨ì–´ ì„±ê³µ ({time.time() - start_time:.2f}s)")
        return sum(success_counts)