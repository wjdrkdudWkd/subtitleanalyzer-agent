import asyncio
import logging
import time
from typing import List

from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import PydanticOutputParser
from sqlmodel import Session, select
from app.models import WordEntry, JapaneseWordMetadata, WordLearningContent, SubtitleSentence
from app.schemas import WordEnhanceResponse

load_dotenv()

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


class AIWordService:
    def __init__(self):
        self.llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
        self.parser = PydanticOutputParser(pydantic_object=WordEnhanceResponse)
        self.semaphore = asyncio.Semaphore(3)  # ë™ì‹œ ì‹¤í–‰ ì œí•œ

    def _chunk_list(self, lst, n):
        for i in range(0, len(lst), n):
            yield lst[i:i + n]

    async def _process_batch(self, session: Session, chunk: List[tuple], batch_idx: int, total_batches: int):
        """ê°œë³„ ë°°ì¹˜ë¥¼ ì²˜ë¦¬í•˜ê³  ì§„í–‰ í˜„í™©ì„ ë¡œê¹…í•¨"""
        async with self.semaphore:
            batch_start_time = time.time()
            context_list = [{"word": we.base_form, "context": text} for we, text in chunk]
            chunk_size = len(chunk)

            logger.info(f"ğŸš€ [Batch {batch_idx}/{total_batches}] ë‹¨ì–´ {chunk_size}ê°œ ì²˜ë¦¬ ì‹œì‘...")

            prompt = ChatPromptTemplate.from_template(
                "ë„ˆëŠ” ì¼ë³¸ì–´ êµìœ¡ ì „ë¬¸ê°€ì•¼. ì•„ë˜ ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ì™€ ê° ë¬¸ë§¥ì„ ë°”íƒ•ìœ¼ë¡œ ì •ë³´ë¥¼ ì¶”ì¶œí•´ì¤˜.\n"
                "ê·œì¹™: ëª¨ë“  ì½ê¸°ëŠ” íˆë¼ê°€ë‚˜ë¡œë§Œ, JLPT ë“±ê¸‰ì€ N1~N5ë¡œ.\n\n"
                "{format_instructions}\n"
                "ë°ì´í„° ë¦¬ìŠ¤íŠ¸: {context_list}"
            )

            input_data = prompt.format_messages(
                context_list=context_list,
                format_instructions=self.parser.get_format_instructions()
            )

            try:
                response = await self.llm.ainvoke(input_data)
                batch_result = self.parser.parse(response.content)

                # DB ë°˜ì˜ ë¡œì§
                result_map = {item.base_form: item for item in batch_result.words}
                success_count = 0

                for we, _ in chunk:
                    if we.base_form in result_map:
                        data = result_map[we.base_form]
                        we.japanese_metadata.reading = data.reading
                        we.japanese_metadata.jlpt_level = data.jlpt_level

                        content = WordLearningContent(
                            word_entry_id=we.id,
                            meaning=data.meaning,
                            usage_tip=f"JLPT {data.jlpt_level} ìˆ˜ì¤€",
                            generated_example={"ja": data.usage_example, "ko": data.usage_meaning}
                        )
                        session.add(content)
                        success_count += 1

                duration = time.time() - batch_start_time
                logger.info(
                    f"âœ… [Batch {batch_idx}/{total_batches}] ì™„ë£Œ ({success_count}/{chunk_size} ë‹¨ì–´) - ì†Œìš”ì‹œê°„: {duration:.2f}s")
                return success_count

            except Exception as e:
                logger.error(f"âŒ [Batch {batch_idx}/{total_batches}] ì—ëŸ¬ ë°œìƒ: {str(e)}")
                return 0

    async def enhance_words_hybrid(self, session: Session, subtitle_id: int, batch_size: int = 15):
        start_time = time.time()

        # 1. ëŒ€ìƒ ë‹¨ì–´ ì¡°íšŒ
        statement = (
            select(WordEntry, SubtitleSentence.sentence_text)
            .join(JapaneseWordMetadata)
            .join(SubtitleSentence, WordEntry.first_occurrence_id == SubtitleSentence.id)
            .where(WordEntry.subtitle_id == subtitle_id)
            .where(WordEntry.is_valid == True)
            .where(JapaneseWordMetadata.jlpt_level == "WAIT")
        )
        results = session.exec(statement).all()
        total_words = len(results)

        if total_words == 0:
            logger.info(f"â„¹ï¸ ìë§‰ ID {subtitle_id}: ê°•í™”í•  ë‹¨ì–´ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return 0

        # 2. ì²­í‚¹ ë° ë°°ì¹˜ ì¤€ë¹„
        chunks = list(self._chunk_list(results, batch_size))
        total_batches = len(chunks)

        logger.info(f"ğŸ”¥ ìë§‰ ID {subtitle_id}: ì´ {total_words}ê°œ ë‹¨ì–´ ê°•í™” ì‹œì‘ (ì´ {total_batches}ê°œ ë°°ì¹˜)")

        # 3. ë³‘ë ¬ ì²˜ë¦¬ ì‹¤í–‰
        tasks = [
            self._process_batch(session, chunk, i + 1, total_batches)
            for i, chunk in enumerate(chunks)
        ]

        success_counts = await asyncio.gather(*tasks)

        # 4. ìµœì¢… ì €ì¥ ë° ë¡œê·¸
        session.commit()

        total_success = sum(success_counts)
        total_duration = time.time() - start_time
        logger.info(f"ğŸ‰ ì „ì²´ ì‘ì—… ì™„ë£Œ: {total_success}/{total_words} ë‹¨ì–´ ì„±ê³µ - ì´ ì†Œìš”ì‹œê°„: {total_duration:.2f}s")

        return total_success