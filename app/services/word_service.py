import re
import jaconv
from janome.tokenizer import Tokenizer
from sqlmodel import Session, select
from app.models import SubtitleSentence, WordEntry, JapaneseWordMetadata


class WordService:
    def __init__(self):
        self.tokenizer = Tokenizer()

    def _validate_word(self, base_form, pos):
        if not base_form:
            return False, "EMPTY"

            # 1. íŠ¹ìˆ˜ë¬¸ì/ìˆ«ì íŒ¨í„´ ì •ì˜ (ìˆ«ì, ë¬¸ì¥ë¶€í˜¸, ê¸°í˜¸ ë“±)
            # \WëŠ” ë¬¸ìê°€ ì•„ë‹Œ ê²ƒ, [0-9]ëŠ” ìˆ«ì, _ëŠ” ì–¸ë”ë°”
        symbol_pattern = re.compile(r'[0-9\W_]')

        # ğŸ” ì²« ë²ˆì§¸ ë¬¸ìê°€ íŠ¹ìˆ˜ë¬¸ì/ìˆ«ìì¸ì§€ í™•ì¸
        if symbol_pattern.match(base_form[0]):
            # ì²« ê¸€ìê°€ íŠ¹ìˆ˜ë¬¸ìë¼ë©´, 'ì „ì²´ ë¬¸ì'ê°€ íŠ¹ìˆ˜ë¬¸ìì¸ì§€ ê²€ì‚¬
            # all()ì„ ì‚¬ìš©í•´ ì¤‘ê°„ì— ì¼ë°˜ ë¬¸ìê°€ í•˜ë‚˜ë¼ë„ ì„ì—¬ ìˆìœ¼ë©´ True(í†µê³¼)ê°€ ë¨
            if re.fullmatch(r'[\d\W_]+', base_form):
                return False, "ALL_SYMBOLS_OR_NUM"

            # ì²« ê¸€ìëŠ” íŠ¹ìˆ˜ë¬¸ìì§€ë§Œ ë’¤ì— ì¼ë°˜ ë¬¸ìê°€ ì„ì—¬ ìˆë‹¤ë©´?
            # (ì˜ˆ: "!ì•ˆë…•", "1ë“±") -> ìœ íš¨í•œ ë‹¨ì–´ë¡œ ë³´ê³  í†µê³¼ì‹œí‚´

        # 2. ì˜ë¯¸ ì—†ëŠ” í•œ ê¸€ì ê°€ë‚˜ (ì¡°ì‚¬ ì„±ê²©ì´ë‚˜ ë‹¨ìˆœ ê°íƒ„ì‚¬)
        if len(base_form) == 1:
            # íˆë¼ê°€ë‚˜/ê°€íƒ€ì¹´ë‚˜ í•œ ê¸€ìì´ë©´ì„œ ëª…ì‚¬ê°€ ì•„ë‹Œ ê²½ìš°
            if re.match(r'^[ã-ã‚“ã‚¡-ãƒ¶ãƒ¼]$', base_form) and pos != 'åè©':
                return False, "SINGLE_KANA_NOISE"

        return True, None

    def extract_words_from_subtitle(self, session: Session, subtitle_id: int):
        # ë¬¸ì¥ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        statement = select(SubtitleSentence).where(SubtitleSentence.subtitle_id == subtitle_id)
        sentences = session.exec(statement).all()

        # ì¤‘ë³µ ë‹¨ì–´ ë°©ì§€ë¥¼ ìœ„í•œ ë§µ {base_form: WordEntry}
        word_map = {}

        for sent in sentences:
            # í…ìŠ¤íŠ¸ í™•ë³´
            text = sent.sentence_text

            # 2. Janomeìœ¼ë¡œ í˜•íƒœì†Œ ë¶„ì„
            for token in self.tokenizer.tokenize(text):
                base_form = token.base_form
                pos = token.part_of_speech.split(',')[0]

                # ì˜ë¯¸ ìˆëŠ” í’ˆì‚¬(ëª…ì‚¬, ë™ì‚¬, í˜•ìš©ì‚¬, ë¶€ì‚¬)ë§Œ ì¶”ì¶œ
                if pos in ['åè©', 'å‹•è©', 'å½¢å®¹è©', 'å‰¯è©']:
                    if base_form not in word_map:

                        is_valid, skip_reason = self._validate_word(base_form, pos)

                        # 1. ê°€íƒ€ì¹´ë‚˜ ìš”ë¯¸ê°€ë‚˜ë¥¼ íˆë¼ê°€ë‚˜ë¡œ ë³€í™˜
                        # Janome ê²°ê³¼ê°€ '*'ì¸ ê²½ìš°(ê¸°í˜¸ ë“±)ëŠ” ì›ë¬¸(base_form)ì„ ì‚¬ìš©
                        raw_reading = token.reading if token.reading != "*" else base_form
                        hiragana_reading = jaconv.kata2hira(raw_reading)

                        # WordEntry ìƒì„±
                        word_entry = WordEntry(
                            subtitle_id=subtitle_id,
                            first_occurrence_id=sent.id,
                            base_form=base_form,
                            language="ja",
                            part_of_speech=pos,
                            frequency=1,
                            is_valid=is_valid,
                            skip_reason=skip_reason
                        )

                        # ì˜¤ì§ íˆë¼ê°€ë‚˜ì™€ ì¥ìŒ(ãƒ¼)ìœ¼ë¡œë§Œ êµ¬ì„±ëœ íŒ¨í„´
                        hiragana_only_pattern = re.compile(r'^[ã-ã‚“ãƒ¼]+$')

                        is_pure_hiragana = bool(hiragana_only_pattern.match(base_form))
                        metadata = JapaneseWordMetadata(
                            # íˆë¼ê°€ë‚˜ë©´ ê·¸ëŒ€ë¡œ ë„£ê³ , ì•„ë‹ˆë©´(í•œì/ê°€íƒ€ì¹´ë‚˜) AIê°€ ì±„ìš°ë„ë¡ ë¹„ì›Œë‘ 
                            reading=base_form if is_pure_hiragana else None,
                            # íˆë¼ê°€ë‚˜ì—¬ë„ JLPT ë“±ê¸‰ì€ ëª¨ë¥´ë‹ˆ ì¼ë‹¨ WAIT (í˜¹ì€ ë³„ë„ ìƒíƒœê°’)
                            jlpt_level="WAIT",
                            word_entry=word_entry
                        )
                        word_entry.japanese_metadata = metadata
                        word_map[base_form] = word_entry
                    else:
                        # ì´ë¯¸ ë“±ë¡ëœ ë‹¨ì–´ë©´ ë¹ˆë„ìˆ˜ë§Œ ì¦ê°€
                        word_map[base_form].frequency += 1

        # 4. ë²Œí¬ ì €ì¥
        for word in word_map.values():
            session.add(word)

        session.commit()
        return len(word_map)