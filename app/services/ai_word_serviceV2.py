import asyncio
import logging
import time
import json
from collections import defaultdict
from dataclasses import field
from typing import List, Dict, Any

from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import PydanticOutputParser
from pydantic.dataclasses import dataclass
from sqlmodel import Session, select
from app.models import WordEntry, WordLearningContent, SubtitleSentence, ExampleSentence, \
    ExampleTranslation, SubtitleTranslation
from app.schemas import AIWordEnhanceResponseV4

load_dotenv()

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

PROMPT_V3 = """
ë„ˆëŠ” ê¸€ë¡œë²Œ ì¼ë³¸ì–´ êµìœ¡ ì „ë¬¸ê°€ì´ì ì „ë¬¸ ë²ˆì—­ê°€ì•¼. 
ì œê³µëœ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤êµ­ì–´ í•™ìŠµ ì½˜í…ì¸ ë¥¼ ìƒì„±í•´ì¤˜.

[ìˆ˜í–‰ ê³¼ì œ]
1. Context Translation: ì œê³µëœ `contexts` ë¦¬ìŠ¤íŠ¸ì˜ ëª¨ë“  ë¬¸ì¥ì„ í•œêµ­ì–´ë¡œ ì •í™•í•˜ê²Œ ë²ˆì—­í•´.
2. Word Definition: ê° ë‹¨ì–´ì˜ ì˜ë¯¸ë¥¼ contexts ë¬¸ë§¥ì— ë§ê²Œ í’€ì´í•´.
3. Creative Example: `word_groups`ì˜ ë‹¨ì–´ë“¤ì„ í™œìš©í•´ ì €ì‘ê¶Œ ì—†ëŠ” ê³ ìœ í•œ ìƒˆ ë¬¸ì¥ì„ ì°½ì‘í•˜ê³  í•´ì„ì„ ë‹¬ì•„ì¤˜.

[ì ˆëŒ€ ê·œì¹™]
- ì›ë³¸ ìë§‰ì„ ê·¸ëŒ€ë¡œ ì˜ˆë¬¸ìœ¼ë¡œ ì“°ì§€ ë§ˆë¼.
- ëª¨ë“  `word_id`ì™€ `context_id`ë¥¼ ì •í™•íˆ ë§¤ì¹­í•˜ì—¬ ë¦¬í„´í•´ë¼.

{format_instructions}

[ì…ë ¥ ë°ì´í„°]
{payload}
"""

PROMPT_V4 = """
ë‹¹ì‹ ì€ ì¼ë³¸ì–´ êµìœ¡ ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤. ì œê³µëœ `data`ì™€ `groups`ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•™ìŠµ ì½˜í…ì¸ ë¥¼ ìƒì„±í•˜ì„¸ìš”.

[ìˆ˜í–‰ ê³¼ì œ]
1. **Subtitle Translation**: `data`ì˜ ê° `text`ë¥¼ ë¬¸ë§¥ì— ë§ê²Œ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ì„¸ìš”.
2. **Word Definition**: `data.words`ì˜ ê° ë‹¨ì–´ë¥¼ ë¶„ì„í•˜ì„¸ìš”.
   - `mean`(ëœ»)ì€ ë¶€ëª¨ ê°ì²´ì˜ `text` ë¬¸ë§¥ì„ ìµœìš°ì„ ìœ¼ë¡œ ë°˜ì˜í•©ë‹ˆë‹¤.
   - `lv`(JLPT)ëŠ” N1~N5 ë“±ê¸‰ìœ¼ë¡œ íŒì •í•˜ì„¸ìš”.
3. **Group Example Creation**: `groups`ì˜ `wids`ì— í¬í•¨ëœ ëª¨ë“  ë‹¨ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ 'í•˜ë‚˜ì˜ ìƒˆë¡œìš´ ì¼ë³¸ì–´ ë¬¸ì¥'ì„ ì°½ì‘í•˜ì„¸ìš”.
   - **Constraint**: `groups.wids`ëŠ” `data.words.id`ë¥¼ ì°¸ì¡°í•©ë‹ˆë‹¤. ë°˜ë“œì‹œ í•´ë‹¹ IDì˜ ë‹¨ì–´ë“¤ì„ ëª¨ë‘ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.

[ì£¼ì˜ ì‚¬í•­]
- **ID Integrity**: ì…ë ¥ë°›ì€ ëª¨ë“  `id`, `gid`ëŠ” ê²°ê³¼ JSONì—ì„œ ì ˆëŒ€ ë³€ê²½í•˜ê±°ë‚˜ ëˆ„ë½í•˜ì§€ ë§ˆì„¸ìš”. (ë§¤í•‘ ì •í™•ë„ 100% ìœ ì§€)
- **Efficiency**: ë¶€ì—° ì„¤ëª…ì´ë‚˜ ì„œë¡  ì—†ì´, ì§€ì •ëœ JSON í¬ë§·ìœ¼ë¡œë§Œ ì¦‰ì‹œ ì‘ë‹µí•˜ì„¸ìš”.
- **Conciseness**: ì˜ˆë¬¸ í•´ì„(`ko`)ê³¼ ë¡œì§ ì„¤ëª…(`logic`)ì€ í•™ìŠµìê°€ í•œëˆˆì— ì½ê¸° ì¢‹ê²Œ ì§§ê³  ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.

{format_instructions}

[ì…ë ¥ ë°ì´í„°]
{payload}
"""

@dataclass(frozen=True)
class WordDTO:
    """ë‹¨ì–´ ê°œë³„ ì •ë³´"""
    word_id: int
    base_form: str

@dataclass(frozen=True)
class SentenceTaskDTO:
    """ë¬¸ì¥ ë‹¨ìœ„ ì‘ì—… ê·¸ë£¹ (Orchestratorì˜ ê¸°ë³¸ ë‹¨ìœ„)"""
    sentence_id: int
    sentence_text: str
    words: List[WordDTO] = field(default_factory=list)

@dataclass
class BatchProcessingResult:
    """í•œ ë°°ì¹˜ì˜ ì‘ì—… ê²°ê³¼ ë³´ê³ ì„œ"""
    success_count: int
    batch_idx: int
    process_time: float


class AIWordService:
    def __init__(self):
        self.llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
        self.parser = PydanticOutputParser(pydantic_object=AIWordEnhanceResponseV4)
        self.prompt = ChatPromptTemplate.from_template(PROMPT_V4)
        self.semaphore = asyncio.Semaphore(3)  # ë™ì‹œ ì‹¤í–‰ ì œí•œ

    def _fetch_analysis_targets(self, session: Session, subtitle_id: int) -> List[SentenceTaskDTO]:
        statement = (
            select(
                WordEntry,  # [0] WordEntry ê°ì²´ ì „ì²´ (Entity)
                SubtitleSentence.id,  # [1] ë¬¸ì¥ ID (Long/Integer)
                SubtitleSentence.sentence_text  # [2] ë¬¸ì¥ ë‚´ìš© (String)
            )
            .join(
                SubtitleSentence,
                WordEntry.first_occurrence_id == SubtitleSentence.id
            )
            .where(
                WordEntry.subtitle_id == subtitle_id,
                WordEntry.is_valid == True,
                # ì•„ì§ AI ë¶„ì„ì´ ì•ˆ ëœ ë°ì´í„°ë§Œ í•„í„°ë§ (ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§)
                # JapaneseWordMetadata ì¡°ì¸ì´ í•„ìš”í•  ìˆ˜ ìˆìŒ
            )
        )
        results = session.exec(statement).all()

        sentence_map = {}

        for word_obj, s_id, s_text in results:
            if s_id not in sentence_map:
                sentence_map[s_id] = SentenceTaskDTO(
                    sentence_id=s_id,
                    sentence_text=s_text,
                    words=[]
                )

            sentence_map[s_id].words.append(
                WordDTO(
                    word_id=word_obj.id,
                    base_form=word_obj.base_form,
                )
            )

        return list(sentence_map.values())



    async def enhance_words_hybrid(self, session: Session, subtitle_id: int, batch_size: int = 3):
        """
            [Orchestrator]
            ê³„ì¸µí˜• ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ ë³‘ë ¬ë¡œ ë¶„ì„ì„ ì‹¤í–‰í•˜ê³  ìµœì¢… ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
            batch_size = ë¬¸ì¥ ê°œìˆ˜
        """
        start_time = time.time()

        # 1. [FETCH] ê³„ì¸µí˜• DTO ë¦¬ìŠ¤íŠ¸ í™•ë³´
        sentence_tasks = self._fetch_analysis_targets(session, subtitle_id)

        if not sentence_tasks:
            logger.info("âœ… ë¶„ì„í•  ìƒˆë¡œìš´ ë‹¨ì–´ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return 0

        # 2. [CHUNKING] ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë°°ì¹˜ ë¶„í• 
        # ë‹¨ì–´ ê°œìˆ˜ê°€ ì•„ë‹ˆë¼ 'ë¬¸ì¥(Context) ê°œìˆ˜' ê¸°ì¤€ìœ¼ë¡œ ìª¼ê°­ë‹ˆë‹¤.
        chunks = [
            sentence_tasks[i: i + batch_size]
            for i in range(0, len(sentence_tasks), batch_size)
        ]

        total_batches = len(chunks)
        logger.info(f"ğŸ”¥ ë¶„ì„ ì‹œì‘: ì´ {len(sentence_tasks)}ê°œ ë¬¸ì¥ ê·¸ë£¹ / {total_batches}ê°œ ë°°ì¹˜")

        # 3. [PARALLEL EXECUTION] asyncio.gatherë¡œ ë³‘ë ¬ ì²˜ë¦¬
        # ê° ë°°ì¹˜ëŠ” ë…ë¦½ì ì¸ ë¹„ë™ê¸° íƒœìŠ¤í¬ë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.
        batch_tasks = [
            self._process_batch_v3(session, chunk, i + 1, total_batches)
            for i, chunk in enumerate(chunks)
        ]

        # ìë°”ì˜ join()ì²˜ëŸ¼ ëª¨ë“  íƒœìŠ¤í¬ê°€ ëë‚  ë•Œê¹Œì§€ ë¹„ë™ê¸° ëŒ€ê¸°í•©ë‹ˆë‹¤.
        # ê²°ê³¼ê°’ìœ¼ë¡œ ê° ë°°ì¹˜ì˜ ì„±ê³µ ë‹¨ì–´ ê°œìˆ˜ ë¦¬ìŠ¤íŠ¸ê°€ ëŒì•„ì˜µë‹ˆë‹¤.
        # success_counts = await asyncio.gather(*batch_tasks)

        # ëë‚˜ëŠ” ìˆœì„œëŒ€ë¡œ í•˜ë‚˜ì”© ë°˜í™˜ë°›ìŒ (as_completed)
        success_counts = []
        completed_count = 0
        for task in asyncio.as_completed(batch_tasks):
            count = await task  # ì—¬ê¸°ì„œ ê° ë°°ì¹˜ì˜ ê²°ê³¼ê°€ ë‚˜ì˜´
            success_counts.append(count)
            completed_count += 1

            # ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ë ˆë²¨ì—ì„œ ì „ì²´ ì§„í–‰ë¥  ë¡œê¹… ê°€ëŠ¥
            percent = (completed_count / total_batches) * 100
            logger.info(f"ğŸš€ ì „ì²´ ì§„í–‰ë¥ : {percent:.1f}% ({completed_count}/{total_batches} ë°°ì¹˜ ì²˜ë¦¬ë¨)")
            # ì¶”í›„ ì‹¤ì‹œê°„ ì§„í–‰ë¥  í‘œì‹œ SSE

        # 4. [COMMIT & SUMMARY]
        total_success = sum(success_counts)
        session.commit()  # íŠ¸ëœì­ì…˜ ìµœì¢… ì»¤ë°‹

        duration = time.time() - start_time
        logger.info(f"ğŸ‰ ì „ì²´ ì‘ì—… ì™„ë£Œ: {total_success}ê°œ ë‹¨ì–´ ê°•í™” ì„±ê³µ ({duration:.2f}s)")

        return total_success


    async def _process_batch_v3(self, session: Session, chunk: List[SentenceTaskDTO], batch_idx: int, total_batches: int) -> int :
        """
            ê°œë³„ ë°°ì¹˜ë¥¼ ì²˜ë¦¬í•˜ëŠ” ì‹¤ì œ ì‘ì—… ë‹¨ìœ„
        """
        async with self.semaphore: # ë™ì‹œ í˜¸ì¶œ ì œí•œ (Rate Limit ê´€ë¦¬)
            batch_start_time = time.time()

            # 1. AIì—ê²Œ ë³´ë‚¼ í˜ì´ë¡œë“œ ì¡°ë¦½ DTO => JSON
            payload = self._build_ai_payload(chunk)

            input_message = self.prompt.format_messages(
                payload=json.dumps(payload, ensure_ascii=False),
                format_instructions=self.parser.get_format_instructions()
            )

            try:
                response = await self.llm.ainvoke(input_message)
                ai_data = self.parser.parse(response.content)

                # 3. DB ì €ì¥
                # ì„±ê³µí•œ ë‹¨ì–´ ê°œìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
                success_count = self._save_v3_results(session, chunk, ai_data)

                # session.flush()ëŠ” _save_v3_results ë‚´ë¶€ì—ì„œ ìˆ˜í–‰ë©ë‹ˆë‹¤.
                # ìµœì¢… commitì€ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°ê°€ í•©ë‹ˆë‹¤.

                return success_count

            except Exception as e:
                logger.error(f"âŒ [Batch {batch_idx}] ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
                return 0


    def _save_v3_results(self,session: Session, chunk: List[SentenceTaskDTO], ai_data: AIWordEnhanceResponseV4):
        """
            AI ì‘ë‹µ ë°ì´í„°ë¥¼ ì •ê·œí™”ëœ í…Œì´ë¸”ë“¤ì— ë‚˜ëˆ ì„œ ì €ì¥ (Flush í¬í•¨)
        """

        # 1. ë§¤í•‘ ì¤€ë¹„: AIì˜ ê°€ìƒ context_id -> ì‹¤ì œ DB sentence_id
        # SentenceTaskDTO êµ¬ì¡° ë•ë¶„ì— ì•„ì£¼ ì‰½ê²Œ ë§µì„ ë§Œë“­ë‹ˆë‹¤.
        context_id_map = {task.sentence_id: task.sentence_id for task in chunk}

        # 2. ë‹¨ì–´ ê°ì²´ ë§µí•‘ (Updateìš©)
        # ì´ë²ˆ ë°°ì¹˜ì— í¬í•¨ëœ ëª¨ë“  ë‹¨ì–´ ID ë¦¬ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•´ì„œ ë§µí•‘
        word_ids = [w.word_id for task in chunk for w in task.words]
        word_map = {w_id: session.get(WordEntry, w_id) for w_id in word_ids}

        success_count = 0

        # --- (1) ì›ë³¸ ìë§‰ ë²ˆì—­ ì €ì¥ (SubtitleTranslation) ---
        # 1. ì´ë²ˆ ë°°ì¹˜ì— í¬í•¨ëœ ë¬¸ì¥ ID ë¦¬ìŠ¤íŠ¸ í™•ë³´
        sentence_ids = [task.sentence_id for task in chunk]

        # 2. [Select] ì´ë¯¸ ë²ˆì—­ì´ ì¡´ì¬í•˜ëŠ” ë¬¸ì¥ IDë“¤ì„ í•œ ë²ˆì— ì¡°íšŒ (N+1 ë°©ì§€)
        existing_translations = session.exec(
            select(SubtitleTranslation)
            .where(
                SubtitleTranslation.subtitle_sentence_id.in_(sentence_ids),
                SubtitleTranslation.language_code == "ko"
            )
        ).all()

        # ë¹ ë¥¸ ì¡°íšŒë¥¼ ìœ„í•´ ë§µìœ¼ë¡œ ë³€í™˜ {sentence_id: translation_obj}
        trans_map = {t.subtitle_sentence_id: t for t in existing_translations}

        # 3. [Insert or Update]
        for trans in ai_data.trans:
            db_sent_id = context_id_map.get(trans.s_id)
            if not db_sent_id: continue

            if db_sent_id in trans_map:
                # ğŸ’¡ [Update] ì´ë¯¸ ìˆìœ¼ë©´ ë‚´ìš©ë§Œ ê°±ì‹  (Dirty Checking í™œìš©)
                trans_map[db_sent_id].translated_text = trans.tr_ko
            else:
                # ğŸ’¡ [Insert] ì—†ìœ¼ë©´ ìƒˆë¡œ ì¶”ê°€
                new_sub_trans = SubtitleTranslation(
                    subtitle_sentence_id=db_sent_id,
                    language_code="ko",
                    translated_text=trans.tr_ko
                )
                session.add(new_sub_trans)

        # --- (2) ì°½ì˜ì  ì˜ˆë¬¸ ë° í•´ì„ ì €ì¥ (ExampleSentence) ---
        # AIê°€ ì¤€ ì˜ˆë¬¸ ê·¸ë£¹ë³„ë¡œ ì €ì¥í•˜ê³  ìƒì„±ëœ PKë¥¼ ë³´ê´€
        example_id_map = {}  # {ai_group_id: db_example_id}

        for ex in ai_data.exs:
            new_example = ExampleSentence(
                sentence_text=ex.ex_ja
            )
            session.add(new_example)
            session.flush()  # ğŸ’¡ [ID í™•ë³´] DBì— ì¿¼ë¦¬ë¥¼ ë‚ ë ¤ ìë™ ìƒì„±ëœ PKë¥¼ ë°›ì•„ì˜µë‹ˆë‹¤ (JPAì˜ saveAndFlush)

            example_id_map[ex.gid] = new_example.id

            # ì˜ˆë¬¸ì˜ ë²ˆì—­ë„ ì„¸íŠ¸ë¡œ ì €ì¥
            session.add(ExampleTranslation(
                example_id=new_example.id,
                language_code="ko",
                translated_text=ex.ex_ko
            ))

        # --- (3) ë‹¨ì–´ ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ ë° í•™ìŠµ ì½˜í…ì¸  ìƒì„± ---
        for def_res in ai_data.words:
            word_entry = word_map.get(def_res.w_id)
            if not word_entry: continue

            # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ (Dirty Checkingì²˜ëŸ¼ ì‘ë™)
            word_entry.japanese_metadata.reading = def_res.r
            word_entry.japanese_metadata.jlpt_level = def_res.lv

            # í•´ë‹¹ ë‹¨ì–´ê°€ í¬í•¨ëœ AI ì˜ˆë¬¸ ê·¸ë£¹ ID ì°¾ê¸°
            target_group_id = next((ex.gid for ex in ai_data.exs if def_res.w_id in ex.wids), None)

            if target_group_id in example_id_map:
                session.add(WordLearningContent(
                    word_entry_id=word_entry.id,
                    example_id=example_id_map[target_group_id],
                    meaning=def_res.m,
                    usage_tip=f"JLPT {def_res.lv} ìˆ˜ì¤€ì˜ ë‹¨ì–´ì…ë‹ˆë‹¤.",
                    language_code="ko"
                ))
                success_count += 1

        return success_count


    def _build_ai_payload(self, chunk: List[SentenceTaskDTO]) -> Dict[str, Any]:
        """
        ê¸°ì¡´ ì¸í„°ë¦¬ë¹™ ë¡œì§ì„ ìœ ì§€í•˜ë©´ì„œ ì‘ì§‘ë„ ë†’ì€ JSON êµ¬ì¡°ë¡œ í˜ì´ë¡œë“œ ìƒì„±
        """
        # 1. 'data' ì„¹ì…˜ ìƒì„±: ë¬¸ì¥ê³¼ ì†Œì† ë‹¨ì–´ë“¤ì„ í•œ ëª¸ìœ¼ë¡œ ë¬¶ìŒ
        # AIê°€ ë¬¸ì¥(Context)ì„ ë³´ë©´ì„œ ë‹¨ì–´ë¥¼ ë°”ë¡œ í•´ì„í•  ìˆ˜ ìˆê²Œ ì‘ì§‘ë„ë¥¼ ë†’ì…ë‹ˆë‹¤.
        data = [
            {
                "id": task.sentence_id,  # DB ì‹¤ì œ PK ì‚¬ìš©
                "text": task.sentence_text,
                "words": [
                    {"id": w.word_id, "base": w.base_form}
                    for w in task.words
                ]
            }
            for task in chunk
        ]

        # 2. 'groups' ì„¹ì…˜ ìƒì„±: ê¸°ì¡´ ì¸í„°ë¦¬ë¹™ ë¡œì§ ìœ ì§€
        # ë¬¸ì¥ë“¤ì„ ìˆœíšŒí•˜ë©° ë‹¨ì–´ë¥¼ í•˜ë‚˜ì”© ë½‘ì•„ ê³¨ê³ ë£¨ ì„ì¸ ê·¸ë£¹ì„ ë§Œë“­ë‹ˆë‹¤.
        all_word_ids = []
        max_words_in_sentence = max(len(task.words) for task in chunk) if chunk else 0

        for i in range(max_words_in_sentence):
            for task in chunk:
                if i < len(task.words):
                    # AIì—ê²ŒëŠ” ID ë¦¬ìŠ¤íŠ¸ë§Œ ë„˜ê²¨ì„œ ì¶”ë¡  ë¹„ìš©(í† í°)ì„ ì ˆì•½í•©ë‹ˆë‹¤.
                    all_word_ids.append(task.words[i].word_id)

        # 3. ì¶”ì¶œëœ ë‹¨ì–´ IDë“¤ì„ 3ê°œì”© ë¬¶ì–´ì„œ ê·¸ë£¹í™”
        groups = []
        for i in range(0, len(all_word_ids), 3):
            groups.append({
                "gid": (i // 3) + 1,
                "wids": all_word_ids[i: i + 3]
            })

        return {
            "data": data,
            "groups": groups
        }